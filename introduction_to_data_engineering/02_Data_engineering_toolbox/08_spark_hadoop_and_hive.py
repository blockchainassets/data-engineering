'''

Spark, Hadoop and Hive

You've encountered quite a few open source projects in the previous video. There's Hadoop, Hive, and PySpark. It's easy to get confused between these projects.

They have a few things in common: they are all currently maintained by the Apache Software Foundation, and they've all been used for massive parallel processing. Can you spot the differences?

Instructions

Classify the cards to the corresponding software project.

'''

Hadoop:

   -HDFS is part of it
   -MapReduce is part of it
   -Collction of open source packaged for Big Data

PySpark:
   -Python interface for the Spark framework
   -Uses DataFrame abstraction

Hive:
   -Initially used Hadoop MapReduc
   -Is built from the need to use structured queries for paraller processing 
